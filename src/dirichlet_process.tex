% !TeX root = ../main.tex

\section{Dirichlet Process}

\paragraph{Idea}

The Dirichlet Process is the idea of drawing a GMM from a meta-distribution, which can model infinite Gaussian mixtures.

 We need a fitting algorithm (\textbf{Gibbs sampling}) that fits a GMM to the data independent of the number of components. This is based on the \textbf{Chinese restaurant process}, which provides a constructive way of sampling from a Dirichlet process.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{chinese_restaurant_process}
  \caption[Chinese restaurant process]{There are tables in a restaurant (our mixture components). Whenever a new customer comes in (a sample), it chooses the table it most relates to. When the customer is unsatisfied with current offering, he can open up a new table (\href{https://en.wikipedia.org/wiki/Chinese_restaurant_process}{Check the explanation form Wikipedia}).}
\end{figure}

\paragraph{This CRP extends a ``normal'' GMM:}
\begin{enumerate}
	\item If a customer prefers to sit at a new table, this is possible we can add arbitrary many tables
	\item The more people sit on a table  the more attractive it is to the new customers (rich get richer)
\end{enumerate}

\begin{algorithm}[H]
\caption{Gibbs sampling}
A straight forward (probabilistic\footnote{because we sample from the list}) clustering algorithm based on the CRP.
\begin{enumerate}
    \item Init: assign each sample to a cluster (\eg randomly, or do \(k-\)means)
    \item Randomly select a sample \(\vec{x}_i\) from the data
    \item Compute affinity of \(\vec{x}_i\) to each table: \(t_i = \frac{N_i}{N+\alpha} \mathcal{N}(\vec{x}_i, \vec{\mu}_i, \covar_i)\) (Gaussian distance of \(\vec{x}_i\) to \(\mathcal(\vec{\mu}_i, \covar_i)\)), where \(N_i\) is the number of customers on the table (number of samples assigned to \(\mathcal{N}(\vec{\mu}_i, \covar_i)\) and \(N\) is the total number of samples, \(\alpha\) is an ``expansion parameter''
    \item Compute affinity to a new table \(t_0 = \frac{\alpha}{N + \alpha} \cdot \mathcal{N}(\vec{x}_i, \vec{\mu}_0, \covar_0)\)
    \item Collect all affinities \(t_0, \dots, t_T\) in a list, normalize sum to one
    \item Sample from that list a table assignment (all affinities are interpreted as a PDF from which we sample)
    \item Recompute \(\vec{\mu}_i, \covar_i\) for the assigned table
    \item Go to 2, stop after certain number of iterations
\end{enumerate}
\end{algorithm}

\paragraph{Back to the top-down view:}
Using the CRP, we are effectively drawing a GMM using a Dirichlet process. Implicitly, the mixture weights $\beta_i$ match a $Beta(\alpha)$ distribution and the normal distributions are drawn from a hyper-distribution, i.e. $\vartheta_i \sim H(\lambda)$

\paragraph{Stick breaking process:} \href{https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process}{(check the wiki-page)}
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{stick_breaking_process}
    \caption{Illustration of a $Beta$ distribution}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{Beta_distribution}
    \caption{Qualitatively, this is what the $Beta$ distribution looks like}
  \end{subfigure}
  \caption{$Beta$ distribution}
\end{figure}

The expansion parameter $\alpha$ is a prior that influences the number of clusters that will be created.

\paragraph{Observation 1:}
The larger the expansion parameter $\alpha$ is, the more likely it is to draw a number that is close to $0$ form $Beta(1,\alpha)$.

The number that has been drawn form $Beta(1,\alpha)$ is used in the stick breaking process to determine the weight of the $i$-th mixture component $\beta_i$.

\paragraph{Observation 2:}
The number that has been drawn from \(Beta(1,\alpha)\) is used in the stick breaking process to determine the weight of the \(i\)-th mixture component \(\beta_i\).

\paragraph{Stick breaking process (formalized):}

\begin{equation*}
	b_i \sim Beta(1,\alpha)
\end{equation*}

\begin{equation*}
	\beta_i = b_i \prod\limits_{i=l}^{i-1}(1-b_l) = b_i \left(1 - \sum_{l=1}^{i-1} \beta_l\right)
\end{equation*}
